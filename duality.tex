\documentclass[]{article}
\usepackage[portrait, margin=5mm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{listings}
\pagenumbering{gobble}

\renewcommand{\.}{\hskip .75pt}

\renewcommand{\arraystretch}{1.25}

\newcommand{\fin}[1]{[\.#1\.]}

\DeclareMathOperator{\aand}{\;\wedge\;}
\DeclareMathOperator{\st}{,\;}
\DeclareMathOperator{\ex}{\,\exists}

\let\r=\rightarrow
\let\*=\cdot

\begin{document}

\lstset{
	basicstyle=\ttfamily\small,
	literate=
	{→}{{$\rightarrow$}}1
	{∀}{{$\forall$}}1
	{∃}{{$\exists$}}1
	{×}{{$\times$}}1
	{σ}{{$\sigma$}}1
	{τ}{{$\tau$}}1
	{α}{{$\alpha$}}1
	{≠}{{$\neq$}}1
	{≤}{{$\leq$}}1
	{≥}{{$\geq$}}1
	{↔}{{$\iff$}}1
	{¬}{{$\neg$}}1
	{∧}{{$\wedge$}}1
	{∨}{{$\vee$}}1
	{•}{$\bullet$}1
	{·}{$\cdot$}1
	{⬝}{$\cdot$}1
	{ℕ}{{$\mathbb{N}$}}1
	{ℤ}{{$\mathbb{Z}$}}1
	{ₗ}{{$_l$}}1
	{₀}{{$_0$}}1
	{∑}{{$\sum$}}1
	{ᵀ}{{$^\texttt{T}$}}1
	{ᵥ}{{$_v$}}1
	{ₘ}{{$_m$}}1
	{⁻¹}{{$^{-1}$}}1
	{∞}{{$\infty$}}1
	{⊤}{{$\top$}}1
	{⊥}{{$\bot$}}1
	{⟨}{{$\langle$}}1
	{⟩}{{$\rangle$}}1
	{∘}{{$\circ$}}1
}


\title{Duality theory in linear optimization and its extensions\,---\,formally verified}
\date{}
\maketitle


\section{Introduction}

A basic knowledge from linear algebra is that
a system of linear equalities has a solution if and only if
we cannot obtain a contradiction by taking a linear combination of the equalities.
We state this theorem as follows.

\medskip \noindent
\textbf{Theorem (basicLinearAlgebra):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y = 0$ and $b \* y \neq 0$
\end{itemize}
This theorem can be given in much more general settings.
Here, however, this is the only version we provide.
This special case of the Fredholm alternative is not our main focus
but a byproduct of the other theorems we prove.
In particular, we obtain basicLinearAlgebra as an immediate corollary
of the following theorem.

\medskip \noindent
\textbf{Theorem (basicLinearAlgebra\_lt):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y = 0$ and $b \* y < 0$
\end{itemize}
The way we state the theorem exemplifies certain patterns that
permeate through our work. Our results are phrased as
``there are two systems of (in)equalities; exactly one of them has a solution''.
This goes hand-in-hand with our decision to focus mostly on ``symmetric'' Farkas-like theorems.
Note that it must be impossible to satisfy the second system by $y=0$.
Had it been allowed, we would have said nothing about the first system as it would
have to lead to a contradiction every time. The constraint $b \* y < 0$ disqualifies
the zero solution in most of our theorems. Intuitively, it should make sense that
one of the systems is always ``strict'' (to easily see why, consider $I$ and $J$ singletons),
which also means that our two systems will never be ``fully symmetric''.

Farkas (TODO citation) gave a similar characterization for systems of linear equalities
with nonnegative variables.
We state his theorem as follows.

\medskip \noindent
\textbf{Theorem (equalityFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y \ge 0$ and $b \* y < 0$
\end{itemize}
We prove basicLinearAlgebra\_lt by applying equalityFarkas to the matrix $(A~|-\!\! A)$.
However, equalityFarkas will be proved later, from a more general theorem.

Minkowski (TODO citation) similarly established that
a system of linear inequalities has a nonnegative solution if and only if
we cannot obtain a contradiction by taking a nonnegative linear combination of the inequalities.
For reasons that will be apparent later, we give two versions of this theorem.

\medskip \noindent
\textbf{Theorem (inequalityFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $A^T\! \* y \ge 0$ and $b \* y < 0$
\end{itemize}
\textbf{Theorem (inequalityFarkas\_neg):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
Obviously, inequalityFarkas\_neg is an immediate corollary of inequalityFarkas.
We prove inequalityFarkas by applying equalityFarkas 
to the matrix $(1~|~A)$ where $1$ is the identity matrix of type
$(I \times I) \r F$.

The next theorem generalizes equalityFarkas to structures where
multiplication does not have to be commutative.
Furthermore, it supports infinitely many equations.

\medskip \noindent
\textbf{Theorem (coordinateFarkas):}
Let $I$ be any type.
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $A$ be an $R$-linear map from from $(I \r R)$ to $(J \r R)$.
Let $b$ be an $R$-linear map from from $(I \r R)$ to $R$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r R$ such that, for all $w : I \r R$, we have
$ \sum_{j : J}\; (A~w)_j \bullet x_j = b~w $
\item vector $y : I \r R$ such that $A~y \ge 0$ and $b~y < 0$
\end{itemize}
In the next generalization, we replace the partially ordered module $I \r R$ by
a general $R$-module $W$.

\medskip \noindent
\textbf{Theorem (scalarFarkas):}
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $W$ be an $R$-module.
Let $A$ be an $R$-linear map from from $W$ to $(J \r R)$.
Let $b$ be an $R$-linear map from from $W$ to $R$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r R$ such that, for all $w : W$, we have
$ \sum_{j : J}\; (A~w)_j \bullet x_j = b~w $
\item vector $y : W$ such that $A~y \ge 0$ and $b~y < 0$
\end{itemize}
In the most general theorem, stated below, we replace certain occurrences of $R$ by
a linearly ordered $R$-module $V$ whose order respects order on $R$.
This result origins from TODO.

\medskip \noindent
\textbf{Theorem (fintypeFarkasBartl):}
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $W$ be an $R$-module.
Let $V$ be a linearly ordered $R$-module. We furthermore require 
monotonicity of scalar multiplication by nonnegative elements on the left.
Let $A$ be an $R$-linear map from $W$ to $(J \r R)$.
Let $b$ be an $R$-linear map from $W$ to $V$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector family $x : J \r V$ such that, for all $w : W$, we have
$ \sum_{j : J}\; (A~w)_j \bullet x_j = b~w $
\item vector $y : W$ such that $A~y \ge 0$ and $b~y < 0$
\end{itemize}
In the last branch, $A~y \ge 0$ uses the partial order on $(J \r R)$ whereäs
$b~y < 0$ uses the linear order on $V$.
Note that fintypeFarkasBartl subsumes scalarFarkas (as well as the other versions based on equality),
since $R$ can be viewed as a linearly ordered module over itself.

\newpage

\section{Extensions}

Until now, we have talked about known results.
What follows is a new extension of the theory.

\medskip \noindent
\textbf{Definition:}
Let $F$ be a linearly ordered field.
We define an \textbf{extended} linearly ordered field $F_\infty$ as
$F \cup \{ \bot, \top \}$ with the following properties.
Let $p$ and $q$ be numbers from $F$.
We have $\bot < p < \top$.
We define addition, scalar action, and negation on $F_\infty$ as follows:
\begin{center}
	\begin{tabular}{ c || c | c | c | }
		+ & $\bot$ & $q$ & $\top$  \\
		\hline\hline
		$\bot$ & $\bot$ & $\bot$ & $\bot$  \\ 
		\hline
		$p$ & $\bot$ & $p\!+\!q$ & $\top$  \\ 
		\hline
		$\top$ & $\bot$ & $\top$ & $\top$ \\ 
		\hline
	\end{tabular}
	\qquad\qquad\qquad
	\begin{tabular}{ c || c | c | c | }
		$\bullet$ & $\bot$ & $q$ & $\top$  \\
		\hline\hline
		$0$ & $\bot$ & $0$ & $0$  \\ 
		\hline
		$p>0$ & $\bot$ & $p \cdot q$ & $\top$  \\ 
		\hline
	\end{tabular}
	\qquad\qquad\qquad
	\begin{tabular}{ c || c | c | c }
	$-$ & $\bot$ & $q$ & $\top$  \\
	\hline\hline
	& $\top$ & $-q$ & $\bot$  
	\end{tabular}
\end{center}
When we talk about elements of $F_\infty$,
we say that values from $F$ are \textbf{finite}.

Informally speaking, $\top$ represents the positive infinity,
$\bot$ represents the negative infinity, and we say that
$\bot$ is ``stronger'' than $\top$ in~all arithmetic operations.
The surprising parts are $\bot + \top = \bot$ and $0 \.\bullet \bot = \bot$.
Because of them, $F_\infty$ is not a field.
In fact, $F_\infty$ is not even a group. 
However, $F_\infty$ is still a densely linearly ordered abelian monoid
with characteristic zero.

\medskip \noindent
\textbf{Theorem (extendedFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F_\infty$.
Let $b$ be a vector of type $I \r F_\infty$.
Assume that $A$ does not have $\bot$ and $\top$ in the same row.
Assume that $A$ does not have $\bot$ and $\top$ in the same column.
Assume that $A$ does not have $\top$ in any row where $b$ has $\top$.
Assume that $A$ does not have $\bot$ in any row where $b$ has~$\bot$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
Note that extendedFarkas looks pretty much like equalityFarkas\_neg and,
in certain sense, generalizes it.

Next we define an extended notion of linear program, i.e.,
linear programming over extended linearly ordered fields.
The implicit intention is that the linear program is to be minimized.

\medskip \noindent
\textbf{Definition:}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F_\infty$,
let $b$ be a vector of type $I \r F_\infty$,
and $c$ be a vector of type $J \r F_\infty$
such that the following six conditions hold:
\begin{itemize}
\item $A$ does not have $\bot$ and $\top$ in the same row
\item $A$ does not have $\bot$ and $\top$ in the same column
\item $b$ does not contain $\bot$
\item $c$ does not contain $\bot$
\item $A$ does not have $\top$ in any row where $b$ has $\top$
\item $A$ does not have $\bot$ in any column where $c$ has $\top$
\end{itemize}
We say that $P = (A, b, c)$ is a \textbf{linear program} over $F_\infty$
whose constraints are indexed by $I$ and variables are indexed by $J$.
We say that a nonnegative vector $x : J \r R$ is
a \textbf{solution} to $P$ if and only if $A \* x \le b$.
We say that $P$ \textbf{reaches} an objective value $r$
if and only if there exists $x$ such that $x$ is a solution to $P$
and $c \* x = r$.
We say that $P$ is \textbf{feasible} if and only if $P$ reaches a finite\footnote{
It would be perhaps more natural to say that $P$ reaches a value different
from $\top$. However, since $\bot$ cannot be reached because of the way
linear programming is defined, it is equivalent to our definition by
reaching a finite value.} value.
We say that $P$ is \textbf{bounded by} a finite value $r$ if and only if,
for every value $p$ reached by $P$, we have $r \le p$.
We say that $P$ is \textbf{unbounded} if and only if there is no finite value $r$
such that $P$ is bounded by $r$.
We say that the linear program $(-A^T, c, b)$ is the \textbf{dual} of $P$.

\medskip \noindent
\textbf{Theorem (weakDuality):}
Let $F$ be a linearly ordered field.
Let $P$ be a linear program over $F_\infty$.
If $P$ reaches $p$ and the dual of $P$ reaches $q$,
then $p + q \ge 0$.

\medskip \noindent
\textbf{Definition:}
Let $F$ be a linearly ordered field.
Let $P$ be a linear program over $F_\infty$.
We define the \textbf{optimum} of $P$ as follows.
If $P$ is feasible and unbounded, its optimum is $\bot$.
If $P$ is not feasible, its optimum is $\top$.
In all other cases, we ask whether $P$ reaches a finite value $r$ such that
$P$ is bounded by $r$. If so, its optimum is $r$.
Otherwise, $P$ does not have optimum.\footnote{By the end of the paper, we will
have proved that optimum always exists, i.e., it cannot happen that the set of
objective values reached by $P$ has a finite infimum that is not attained.
However, because we do not have the theorem now, the optimum is defined as a partial function
from linear programs to $F_\infty$.}

\medskip \noindent
\textbf{Theorem (strongDuality):}\footnote{For simplicity,
we rephrased the theorem without mentioning partial functions.
Also note that it would be incorrect to say the following:
$P$ has optimum~$p$, the dual of $P$ has optimum $q$, and $p + q = 0$.
It would fail for unbounded linear programs because the arithmetics
of $F_\infty$ defines $\top + \bot = \bot$.}
Let $F$ be a linearly ordered field.
Let $P$ be a linear program over $F_\infty$.
If $P$ or its dual is feasible (at least one of them),
then there exists $p$ in $F_\infty$ such that
$P$ has optimum $p$ and the dual of $P$ has optimum $-p$.


\end{document}
