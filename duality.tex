\documentclass[]{article}
\usepackage[portrait, margin=5mm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{listings}
\pagenumbering{gobble}

\renewcommand{\.}{\hskip .75pt}

\DeclareMathOperator{\aand}{\;\wedge\;}
\DeclareMathOperator{\st}{,\;}
\DeclareMathOperator{\ex}{\,\exists}

\begin{document}

\let\r=\rightarrow
\let\*=\cdot

\renewcommand{\arraystretch}{1.25}

\lstset{
	basicstyle=\ttfamily\small,
	literate=
	{→}{{$\rightarrow$}}1
	{∀}{{$\forall$}}1
	{∃}{{$\exists$}}1
	{×}{{$\times$}}1
	{σ}{{$\sigma$}}1
	{τ}{{$\tau$}}1
	{≠}{{$\neq$}}1
	{≤}{{$\leq$}}1
	{≥}{{$\geq$}}1
	{↔}{{$\iff$}}1
	{¬}{{$\neg$}}1
	{∧}{{$\wedge$}}1
	{•}{$\bullet$}1
	{·}{$\cdot$}1
	{⬝}{$\cdot$}1
	{ℕ}{{$\mathbb{N}$}}1
	{ₗ}{{$_l$}}1
	{₀}{{$_0$}}1
	{∑}{{$\sum$}}1
	{ᵀ}{{$^\texttt{T}$}}1
	{ᵥ}{{$_v$}}1
}


\title{Duality theory in linear optimization and its extensions\,---\,formally verified}
\date{}
\maketitle


\noindent \textbf{Abstract:}\;
Farkas established that a system of linear inequalities has a solution if and only if we cannot obtain
a contradiction by taking a linear combination of the inequalities.
We state and formally prove several Farkas-like theorems in Lean 4.
Furthermore, we consider a linearly ordered field extended with two special elements denoted by $\bot$ and $\top$
where $\bot$ is below every element and $\top$ is above every element.
We define $\bot + a = \bot = a + \bot$ of all $a$ and we define $\top + b = \top = b + \top$ for all $b \neq \bot$.
For multiplication, we define $\bot \cdot c = \bot = c \cdot \bot$ for every $c \ge 0$ but
$\top \cdot d = \top = d \cdot \top$ only for $d > 0$ because $\top \cdot 0 = 0 = 0 \cdot \top$.
We extend certain Farkas-like theorems to a setting where coefficients are from an extended linearly ordered field.



\section{Introduction}

Rouché, Capelli, Kronecker, Fonténe, Frobenius (TODO citations) established that
a system of linear equalities has a solution if and only if
we cannot obtain a contradiction by taking a linear combination of the equalities.
We state their theorem as follows.

\medskip \noindent
\textbf{Theorem (equalityFrobenius):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y = 0$ and $b \* y \neq 0$
\end{itemize}
Geometric interpretation of equalityFrobenius is straightforward.
The column vectors of $A$ generate a hyperplane in the
$|I|$-dimensional Euclidean space that goes contains the origin.
The point $b$ either lies in this hyperplane (in this case, the entries
of $x$ give coefficients which, when applied to the column vectors of $A$,
give a vector from the origin to the point $b$),
or there exists a line through the origin that is orthogonal to
all the column vectors of $A$ (i.e., orthogonal to the entire hyperplane)
such that $b$ projected onto this line falls outside of the origin
(in this case, $y$ gives a direction of this line), i.e., to a different point
from where all column vectors of $A$ get projected.

This theorem can be given in much more general settings.
In our paper, however, this is the only version we provide.
This staple of linear algebra is not our main focus
but a byproduct of the other theorems we prove.
In particular, we obtain equalityFrobenius as an immediate corollary
of the following theorem.

\medskip \noindent
\textbf{Theorem (equalityFrobenius\_lt):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y = 0$ and $b \* y < 0$
\end{itemize}
The way we state the theorem exemplifies certain patterns that
permeate through our work. Our results are phrased as
``there are two systems of (in)equalities; exactly one of them has a solution''.
This goes hand-in-hand with our decision to focus mostly on ``symmetric'' Farkas-like theorems.
Note that it must be impossible to satisfy the second system by $y=0$.
Had it been allowed, we would have said nothing about the first system as it would
have to lead to a contradiction every time. The constraint $b \* y < 0$ disqualifies
the zero solution in most of our theorems. Intuitively, it should make sense that
one of the systems is always ``strict'', which also means that our two systems will
never be ``fully symmetric''.

Farkas (TODO citation) gave a similar characterization for systems of linear equalities
with nonnegative variables.
We state his theorem as follows.

\medskip \noindent
\textbf{Theorem (equalityFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y \ge 0$ and $b \* y < 0$
\end{itemize}
Geometric interpretation of equalityFarkas is easy.
The column vectors of $A$ generate a cone in the
$|I|$-dimensional Euclidean space from the origin towards some infinity.
The point $b$ either lies inside this cone (in this case, the entries
of $x$ give nonnegative coefficients which,
when applied to the column vectors of $A$,
give a vector from the origin to the point $b$),
or there exists a hyperplane that contains the origin and that
strictly separates $b$ from given cone
(in this case, $y$ gives a normal vector of this hyperplane).

We prove equalityFrobenius\_lt by applying equalityFarkas to the matrix $(A~|-\!\!A)$.
However, equalityFarkas will be proved later, from a more general theorem.

Minkowski (TODO citation) similarly established that
a system of linear inequalities has a nonnegative solution if and only if
we cannot obtain a contradiction by taking a nonnegative linear combination of the inequalities.
For reasons that will be apparent later, we give two versions of this theorem.

\medskip \noindent
\textbf{Theorem (inequalityFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $A^T\! \* y \ge 0$ and $b \* y < 0$
\end{itemize}
Geometric interpretation of inequalityFarkas is a bit harder.
The column vectors of $A$ generate a cone in the
$|I|$-dimensional Euclidean space from the origin to infinity.
The point $b$ determines an orthogonal cone that starts in $b$ and goes to
negative infinity in the direction of all coordinate axes.
Either these two cones intersect (in this case, the entries
of $x$ give nonnegative coefficients which,
when applied to the column vectors of $A$,
give a vector from the origin to the intersection),
or there exists a hyperplane that contains the origin and that
strictly separates $b$ from the cone generated by $A$ but
does not cut through the positive orthant, i.e., the origin
is the only nonnegative point contained in the hyperplane
(in this case, $y$ gives a normal vector of this hyperplane).

\medskip \noindent
\textbf{Theorem (inequalityFarkas\_neg):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
Obviously, inequalityFarkas\_neg is an immediate corollary of inequalityFarkas.
We prove inequalityFarkas by applying equalityFarkas 
to the matrix $(1~|~A)$ where $1$ is the identity matrix of type
$(I \times I) \r F$.

The next theorem generalizes equalityFarkas to structures where
multiplication does not have to be commutative.
Furthermore, it supports infinitely many equations.

\medskip \noindent
\textbf{Theorem (coordinateFarkas):}
Let $I$ be any type.
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $A$ be an $R$-linear map from from $(I \r R)$ to $(J \r R)$.
Let $b$ be an $R$-linear map from from $(I \r R)$ to $R$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r R$ such that, for all $w : I \r R$, we have
$ \sum_{j : J}\; (A~w)_j \bullet x_j = b~w $
\item vector $y : I \r R$ such that $A~y \ge 0$ and $b~y < 0$
\end{itemize}
In the next generalization, we replace the partially ordered module $I \r R$ by
a general $R$-module $W$.

\medskip \noindent
\textbf{Theorem (scalarFarkas):}
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $W$ be an $R$-module.
Let $A$ be an $R$-linear map from from $W$ to $(J \r R)$.
Let $b$ be an $R$-linear map from from $W$ to $R$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r R$ such that, for all $w : W$, we have
$ \sum_{j : J}\; (A~w)_j \bullet x_j = b~w $
\item vector $y : W$ such that $A~y \ge 0$ and $b~y < 0$
\end{itemize}
In the most general theorem, stated below, we replace certain occurrences of $R$ by
a linearly ordered $R$-module $V$ whose order respects order on $R$.
This result origins from TODO.

\medskip \noindent
\textbf{Theorem (fintypeFarkasBartl):}
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $W$ be an $R$-module.
Let $V$ be a linearly ordered $R$-module\footnote{We furthermore require 
monotonicity of scalar multiplication by nonnegative elements on the left.}.
Let $A$ be an $R$-linear map from $W$ to $(J \r R)$.
Let $b$ be an $R$-linear map from $W$ to $V$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative bivector $x : J \r V$ such that, for all $w : W$, we have
$ \sum_{j : J}\; (A~w)_j \bullet x_j = b~w $
\item vector $y : W$ such that $A~y \ge 0$ and $b~y < 0$
\end{itemize}
In the last branch, $A~y \ge 0$ uses the partial order\footnote{The order on $(J \r R)$ is
always the coordinate-wise application of $R$'s linear order.} on $(J \r R)$ whereäs
$b~y < 0$ uses the linear order\footnote{In case $V$ has finite dimension, you can choose
an arbitrary direction and project vectors from $V$ onto it, or you can order elements of
$V$ lexicographically.} on $V$.
Note that fintypeFarkasBartl subsumes scalarFarkas (as well as the other versions based on equality),
since $R$ can be viewed as a linearly ordered module over itself.
We prove fintypeFarkasBartl in Section TODO, which is where the heavy lifting comes.

Until now, we have talked about known results.
What follows is a new extension of the theory.

\medskip \noindent
\textbf{Definition:}
Let $F$ be a linearly ordered field.
We define an \textbf{extended} linearly ordered field $F_\infty$ as
$F \cup \{ \bot, \top \}$ with the following properties.
Let $p$ and $q$ be numbers from $F$.
We have $\bot < p < \top$.
We define addition and scalar action on $F_\infty$ as follows:
\begin{center}
	\begin{tabular}{ c || c | c | c | }
		+ & $\bot$ & $q$ & $\top$  \\
		\hline\hline
		$\bot$ & $\bot$ & $\bot$ & $\bot$  \\ 
		\hline
		$p$ & $\bot$ & $p\!+\!q$ & $\top$  \\ 
		\hline
		$\top$ & $\bot$ & $\top$ & $\top$ \\ 
		\hline
	\end{tabular}
	\qquad\qquad\qquad
	\begin{tabular}{ c || c | c | c | }
		$\bullet$ & $\bot$ & $q$ & $\top$  \\
		\hline\hline
		$0$ & $\bot$ & $0$ & $0$  \\ 
		\hline
		$p>0$ & $\bot$ & $p \cdot q$ & $\top$  \\ 
		\hline
	\end{tabular}
\end{center}
Informally speaking, $\top$ represents the positive infinity,
$\bot$ represents the negative infinity, and we say that
$\bot$ is ``stronger'' than $\top$ in~all arithmetic operations.
The surprising parts are $\bot + \top = \bot$ and $0 \.\bullet \bot = \bot$.
Because of that, $F_\infty$ is not a field.
However, $F_\infty$ is still a densely linearly ordered abelian monoid
with characteristic zero.

\medskip \noindent
\textbf{Theorem (extendedFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F_\infty$.
Let $b$ be a vector of type $I \r F_\infty$.
Assume that $A$ does not have $\bot$ and $\top$ in the same row.
Assume that $A$ does not have $\bot$ and $\top$ in the same column.
Assume that $A$ does not have $\top$ in any row where $b$ has $\top$.
Assume that $A$ does not have $\bot$ in any row where $b$ has~$\bot$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
Note that extendedFarkas looks pretty much like equalityFarkas\_neg and,
in certain sense, generalizes it. Indeed, in Section TODO, we prove
extendedFarkas using equalityFarkas\_neg and some additional machinery.

\medskip \noindent
\textbf{Definition:}
Let $I$ and $J$ be finite types.
Let $R$ be a a partially ordered semiring.
Let $A$ be a matrix of type $(I \times J) \r R$.
Let $b$ be a vector of type $I \r R$.
Let $c$ be a vector of type $J \r R$.
We say that $P = (A, b, c)$ is a \textbf{linear program} over $R$.
We say that its \textbf{constraints} are indexed by $I$.
We say that its \textbf{variables} are indexed by $J$.
We say that a nonnegative vector $x : J \r R$ is
a \textbf{solution} to $P$ if and only if $A \* x \le b$.
We say that $P$ \textbf{reaches} an objective value $r$
if and only if there exists $x$ such that $x$ is a solution to $P$
and $c \* x = r$.

TODO define feasibility and dualize.

TODO state weakDuality and strongDuality.

\section{Formalization}

We distinguish two types of vectors; implicit vectors and explicit vectors.
Implicit vectors are members of a vector space; they don't have any internal structure.
Explicit vectors are functions from coordinates to values.
The set of coordinates needn't be ordered.
Matrices live next to explicit vectors. They are also functions; they take a row index
and a column index and they output a value at the given spot.
Neither the row indices nor the column vertices are required to form an ordered set.
That's why multiplication between matrices and vectors is defined only in structures
where addition forms a commutative semigroup. Consider the following example:
$$
\begin{pmatrix}
	1 & 2 & 3 \\
	4 & 5 & 6 \\
\end{pmatrix}
\*
\begin{pmatrix}
	7 \\ 8 \\ 9
\end{pmatrix}
=
\begin{pmatrix}
	? \\ \_
\end{pmatrix}
$$
We don't know whether the value at the question mark is equal to
$ (1 \* 7 + 2 \* 8) + 3 \* 9 $ or to
$ (2 \* 8 + 1 \* 7) + 3 \* 9 $ or to
any other ordering of summands.
This is why commutativity of addition is necessary for the definition to be valid.
On the other hand, we don't assume any property of multiplication in the
definition of multiplication between matrices and vectors; they don't even
have to be of the same type; we only require the elements of the vector
to have an action on the elements of the matrix (this is not a typo -- normally,
we would want matrices to have an action on vectors -- not in our work).

TODO.


\section {Proving the Farkas-Bartl theorem}

TODO.


\section {Extended Farkas theorem}

\textbf{Theorem (extendedFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F_\infty$.
Let $b$ be a vector of type $I \r F_\infty$.
Assume that $A$ does not have $\bot$ and $\top$ in the same row.
Assume that $A$ does not have $\bot$ and $\top$ in the same column.
Assume that $A$ does not have $\top$ in any row where $b$ has $\top$.
Assume that $A$ does not have $\bot$ in any row where $b$ has~$\bot$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
(restated) \\
Proof idea:
We need to do the following steps in given order:
\begin{enumerate}
\item Delete all rows of both $A$ and $b$ where $A$ has $\bot$ or $b$ has $\top$
(they are tautologies).
\item Delete all columns of $A$ that contain $\top$
(they force respective variables to be zero).
\item If $b$ contains $\bot$, then $A \* x \le b$ cannot be satisfied,
but $y = 0$ satisfies $(-A^T) \* y \le 0$ and $b \* y < 0$. Stop here.
\item Assume there is no $\bot$ in $b$. Use inequalityFarkas\_neg.
In either case, extend $x$ or $y$ with zeros on	all deleted positions.
\end{enumerate}

\subsection{Counterexamples}

If $A$ has $\bot$ and $\top$ in the same row, it may happen that both $x$ and $y$ exist:
$$
A =
\begin{pmatrix}
	\bot & \top \\
	0 & -1
\end{pmatrix}
\qquad \qquad
b = \begin{pmatrix}	0 \\ -1 \end{pmatrix}
\qquad \qquad
x = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
\qquad \qquad
y = \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$
If $A$ has $\bot$ and $\top$ in the same column, it may happen that both $x$ and $y$ exist:
$$
A =
\begin{pmatrix}
	\bot \\
	\top
\end{pmatrix}
\qquad \qquad
b = \begin{pmatrix}	-1 \\ 0 \end{pmatrix}
\qquad \qquad
x = \begin{pmatrix} 0 \end{pmatrix}
\qquad \qquad
y = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
$$
If $A$ has $\top$ in a row where $b$ has $\top$, it may happen that both $x$ and $y$ exist:
%$x$ such that $A \* x \le b$ and 
%$y$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$ exist:
$$
A =
\begin{pmatrix}
	\top \\
	-1
\end{pmatrix}
\qquad \qquad
b = \begin{pmatrix}	\top \\	-1 \end{pmatrix}
\qquad \qquad
x = \begin{pmatrix}	1 \end{pmatrix}
\qquad \qquad
y = \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$
If $A$ has $\bot$ in a row where $b$ has $\bot$, it may happen that both $x$ and $y$ exist:
$$
A =
\begin{pmatrix}
	\bot
\end{pmatrix}
\qquad \qquad
b = \begin{pmatrix}	\bot \end{pmatrix}
\qquad \qquad
x = \begin{pmatrix}	1 \end{pmatrix}
\qquad \qquad
y = \begin{pmatrix}	0 \end{pmatrix}
$$

\section {Proving the (extended) strong LP duality}

TODO.


\section {Related work}

TODO.


\section {Conclusion}

TODO.


\end{document}
