\documentclass[]{article}
\usepackage[portrait, margin=5mm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{listings}
\pagenumbering{gobble}

\renewcommand{\.}{\hskip .75pt}

\DeclareMathOperator{\aand}{\;\wedge\;}
\DeclareMathOperator{\st}{,\;}
\DeclareMathOperator{\ex}{\,\exists}

\begin{document}

\let\r=\rightarrow
\let\*=\cdot

\renewcommand{\arraystretch}{1.25}

\lstset{
	basicstyle=\ttfamily\small,
	literate=
	{→}{{$\rightarrow$}}1
	{∀}{{$\forall$}}1
	{∃}{{$\exists$}}1
	{×}{{$\times$}}1
	{σ}{{$\sigma$}}1
	{τ}{{$\tau$}}1
	{≠}{{$\neq$}}1
	{≤}{{$\leq$}}1
	{≥}{{$\geq$}}1
	{↔}{{$\iff$}}1
	{¬}{{$\neg$}}1
	{∧}{{$\wedge$}}1
	{•}{$\bullet$}1
	{·}{$\cdot$}1
	{⬝}{$\cdot$}1
	{ℕ}{{$\mathbb{N}$}}1
	{ₗ}{{$_l$}}1
	{₀}{{$_0$}}1
	{∑}{{$\sum$}}1
	{ᵀ}{{$^\texttt{T}$}}1
	{ᵥ}{{$_v$}}1
}

\noindent \textbf{Abstract:}\;
Farkas established that a system of linear inequalities has a solution if and only if we cannot obtain
a contradiction by taking a linear combination of the inequalities.
We state and formally prove several Farkas-like theorems in Lean 4.
Furthermore, we consider a linearly ordered field extended with two special elements denoted by $\bot$ and $\top$
where $\bot$ is below every element and $\top$ is above every element.
We define $\bot + a = \bot = a + \bot$ of all $a$ and we define $\top + b = \top = b + \top$ for all $b \neq \bot$.
For multiplication, we define $\bot \cdot c = \bot = c \cdot \bot$ for every $c \ge 0$ but
$\top \cdot d = \top = d \cdot \top$ only for $d > 0$ because $\top \cdot 0 = 0 = 0 \cdot \top$.
We extend certain Farkas-like theorems to a setting where coefficients are from an extended linearly ordered field.



\section{Introduction}

Rouché, Capelli, Kronecker, Fonténe, Frobenius (TODO citations) established that
a system of linear equalities has a solution if and only if
we cannot obtain a contradiction by taking a linear combination of the equalities.
We state their theorem as follows.

\medskip \noindent
\textbf{Theorem (equalityFrobenius):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y = 0$ and $b \* y \neq 0$
\end{itemize}
This theorem can be given in much more general settings.
In our paper, however, this is the only version we provide.
This fundamental staple of linear algebra is not our main focus,
but a byproduct of the other theorems we prove.
The way we state the theorem exemplifies certain patterns that
permeate TODO.

Farkas (TODO citation) gave a similar characterization for systems of linear equalities
with nonnegative variables.
We state his theorem as follows.

\medskip \noindent
\textbf{Theorem (equalityFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $A^T\! \* y \ge 0$ and $b \* y < 0$
\end{itemize}
For reasons that will be apparent later, we give another version of the last theorem.

\medskip \noindent
\textbf{Theorem (equalityFarkas\_neg):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x = b$
\item vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
The proof of equalityFarkas\_neg will come later. However, we note that equalityFarkas is
an immediate corollary of equalityFarkas\_neg and that equalityFrobenius is proved by applying
equalityFarkas to the matrix $(A~|-\!\!A)$.

Minkowski (TODO citation) similarly established that
a system of linear inequalities has a nonnegative solution if and only if
we cannot obtain a contradiction by taking a nonnegative linear combination of the inequalities.
We state his theorem as follows.

\medskip \noindent
\textbf{Theorem (inequalityFarkas):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $A^T\! \* y \ge 0$ and $b \* y < 0$
\end{itemize}
Again, we provide one more version that will make sense later.

\medskip \noindent
\textbf{Theorem (inequalityFarkas\_neg):}
Let $I$ and $J$ be finite types.
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F$.
Let $b$ be a vector of type $I \r F$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
Again, inequalityFarkas is an immediate corollary of inequalityFarkas\_neg.
And as for inequalityFarkas\_neg, we prove it by  applying equalityFarkas\_neg 
to the matrix $(1~|~A)$ where $1$ is the identity matrix of type
$(I \times I) \r F$.

The next theorem generalizes equalityFarkas\_neg to structures where
multiplication does not have to be commutative.
Furthermore, we can have infinitely many equations.

\medskip \noindent
\textbf{Theorem (coordinateFarkas):}
Let $I$ be any type.
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $A$ be an $R$-linear map from from $(I \r R)$ to $(J \r R)$.
Let $b$ be an $R$-linear map from from $(I \r R)$ to $R$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r R$ such that, for all $w : I \r R$, we have
$ \sum_{j : J}\; (A~w)_j \cdot x_j = b~w $
\item vector $y : I \r R$ such that $A~y \le 0$ and $b~y > 0$
\end{itemize}
In the next generalization, we replace the partially ordered module $I \r R$ by
a general $R$-module $W$.

\medskip \noindent
\textbf{Theorem (scalarFarkas):}
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $W$ be an $R$-module.
Let $A$ be an $R$-linear map from from $W$ to $(J \r R)$.
Let $b$ be an $R$-linear map from from $W$ to $R$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r R$ such that, for all $w$ in $W$, we have
$ \sum_{j : J}\; (A~w)_j \cdot x_j = b~w $
\item vector $y : W$ such that $A~y \le 0$ and $b~y > 0$
\end{itemize}
In the most general theorem, stated below, we replace certain occurrences of $R$ by
a linearly ordered $R$-module $V$ whose ordering respects TODO.
This result origins from TODO.

\medskip \noindent
\textbf{Theorem (fintypeFarkasBartl):}
Let $J$ be a finite type.
Let $R$ be a linearly ordered division ring.
Let $W$ be an $R$-module.
Let $V$ be a linearly ordered $R$-module\footnote{We furthermore require PosSMulMono R V,
which means TODO.}.
Let $A$ be an $R$-linear map from from $W$ to $(J \r R)$.
Let $b$ be an $R$-linear map from from $W$ to $V$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative (TODO terminology???) bivector $x : J \r V$ such that, for all $w$ in $W$, we have
$ \sum_{j : J}\; (A~w)_j \cdot x_j = b~w $
\item vector $y : W$ such that $A~y \le 0$ and $b~y > 0$
\end{itemize}
In the last branch, $A~y \le 0$ uses the partial order\footnote{The order on $(J \r R)$ is
always the coordinate-wise application of $R$'s linear order.} on $(J \r R)$ whereäs
$b~y > 0$ uses the linear order\footnote{In case $V$ has finite dimension, you can choose
an arbitrary direction and project vectors from $V$ onto it, or you can order elements of
$V$ lexicographically.} on $V$.
Note that fintypeFarkasBartl subsumes scalarFarkas (as well as the other versions based on equality),
since $R$ can be viewed as a linearly ordered module over itself.
We prove fintypeFarkasBartl in Section TODO, which is where the heavy lifting comes.

Until now, we have talked about known results from TODO.
What follows is a new extension of the theory.

\medskip \noindent
\textbf{Definition:}
Let $F$ be a linearly ordered field.
We define an \textbf{extended} linearly ordered field $F_\infty$ as
$F \cup \{ \bot, \top \}$ with the following properties.
Let $p$ and $q$ be numbers from $F$.
We have $\bot < p < \top$.
We define addition and scalar action on $F_\infty$ as follows:
\begin{center}
	\begin{tabular}{ c || c | c | c | }
		+ & $\bot$ & $q$ & $\top$  \\
		\hline\hline
		$\bot$ & $\bot$ & $\bot$ & $\bot$  \\ 
		\hline
		$p$ & $\bot$ & $p\!+\!q$ & $\top$  \\ 
		\hline
		$\top$ & $\bot$ & $\top$ & $\top$ \\ 
		\hline
	\end{tabular}
	\qquad\qquad\qquad
	\begin{tabular}{ c || c | c | c | }
		$\bullet$ & $\bot$ & $q$ & $\top$  \\
		\hline\hline
		$0$ & $\bot$ & $0$ & $0$  \\ 
		\hline
		$p>0$ & $\bot$ & $p \cdot q$ & $\top$  \\ 
		\hline
	\end{tabular}
\end{center}
Informally speaking, $\top$ represents the positive infinity,
$\bot$ represents the negative infinity, and we say that
$\bot$ is ``stronger'' than $\top$ in~all arithmetic operations.
The surprising parts are $\bot + \top = \bot$ and $0 \.\bullet \bot = \bot$.
Because of that, $F_\infty$ is not a field.
However, $F_\infty$ is still a densely linearly ordered abelian monoid
with characteristic zero.

\medskip \noindent
\textbf{Theorem (extendedFarkas):}
Let $I$ and $J$ be finite types.
%where $I$ is the set of row indices, $J$ is the set of column indices,
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F_\infty$.
Let $b$ be a vector of type $I \r F_\infty$.
Assume that $A$ does not have $\bot$ and $\top$ in the same row.
Assume that $A$ does not have $\bot$ and $\top$ in the same column.
Assume that $A$ does not have $\top$ in any row where $b$ has $\top$.
Assume that $A$ does not have $\bot$ in any row where $b$ has~$\bot$.
Exactly one of the following exists:
\begin{itemize}
\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
Note that extendedFarkas looks pretty much like equalityFarkas\_neg and,
in certain sense, generalizes it. Indeed, in Section TODO, we prove
extendedFarkas using equalityFarkas\_neg and some additional machinery.

\medskip \noindent
\textbf{Definition:}
Let $I$ and $J$ be finite types.
Let $R$ be a a partially ordered semiring.
Let $A$ be a matrix of type $(I \times J) \r R$.
Let $b$ be a vector of type $I \r R$.
Let $c$ be a vector of type $J \r R$.
We say that $P = (A, b, c)$ is a \textbf{linear program} over $R$.
We say that its \textbf{constraints} are indexed by $I$.
We say that its \textbf{variables} are indexed by $J$.
We say that a nonnegative vector $x : J \r R$ is
a \textbf{solution} to $P$ if and only if $A \* x \le b$.
We say that $P$ \textbf{reaches} an objective value $r$
if and only if there exists $x$ such that $x$ is a solution to $P$
and $c \* x = r$.

TODO feasibility, dualize, weakDuality, strongDuality.

\section{Preliminaries}

We distinguish two types of vectors; implicit vectors and explicit vectors.
Implicit vectors are members of a vector space; they don't have any internal structure.
Explicit vectors are functions from coordinates to values.
The set of coordinates needn't be ordered.
Matrices live next to explicit vectors. They are also functions; they take a row index
and a column index and they output a value at the given spot.
Neither the row indices nor the column vertices are required to form an ordered set.
That's why multiplication between matrices and vectors is defined only in structures
where addition forms a commutative semigroup. Consider the following example:
$$
\begin{pmatrix}
	1 & 2 & 3 \\
	4 & 5 & 6 \\
\end{pmatrix}
\*
\begin{pmatrix}
	7 \\ 8 \\ 9
\end{pmatrix}
=
\begin{pmatrix}
	? \\ \_
\end{pmatrix}
$$
We don't know whether the value at the question mark is equal to
$ (1 \* 7 + 2 \* 8) + 3 \* 9 $ or to
$ (2 \* 8 + 1 \* 7) + 3 \* 9 $ or to
any other ordering of summands.
This is why commutativity of addition is necessary for the definition to be valid.
On the other hand, we don't assume any property of multiplication in the
definition of multiplication between matrices and vectors; they don't even
have to be of the same type; we only require the elements of the vector
to have an action on the elements of the matrix (this is not a typo -- normally,
we would want matrices to have an action on vectors -- not in our work).

TODO.


\section {Proving the Farkas-Bartl theorem}

TODO.


\section {Extended Farkas theorem}

\textbf{Theorem (extendedFarkas):}
Let $I$ and $J$ be finite types.
%where $I$ is the set of row indices, $J$ is the set of column indices,
Let $F$ be a linearly ordered field.
Let $A$ be a matrix of type $(I \times J) \r F_\infty$.
Let $b$ be a vector of type $I \r F_\infty$.
Assume that $A$ does not have $\bot$ and $\top$ in the same row.
Assume that $A$ does not have $\bot$ and $\top$ in the same column.
Assume that $A$ does not have $\top$ in any row where $b$ has $\top$.
Assume that $A$ does not have $\bot$ in any row where $b$ has~$\bot$.
Exactly one of the following exists:
\begin{itemize}
	\item nonnegative vector $x : J \r F$ such that $A \* x \le b$
	\item nonnegative vector $y : I \r F$ such that $(-A^T) \* y \le 0$ and $b \* y < 0$
\end{itemize}
(restated) \\
Proof:
We need to do the following steps in given order:
\begin{enumerate}
	\item Delete all rows of both $A$ and $b$ where $A$ has $\bot$ or $b$ has $\top$
	(they are tautologies).
	\item Delete all columns of $A$ that contain $\top$
	(they force respective variables to be zero).
	\item If $b$ contains $\bot$ then the $A \* x \le b$ cannot be satisfied,
	but $y = 0$ satisfies $(-A^T) \* y \le 0$ and $b \* y < 0$. Stop here.
	\item Assume there is no $\bot$ in $b$. Use inequalityFarkas\_neg.
	In either case, extend $x$ or $y$ with zeros on	all deleted positions.
	\item Carefully check all four implications.
\end{enumerate}
We now give a few counterexamples for situations where the preconditions
are not met.
$$
A =
\begin{pmatrix}
	\bot \\
	\top
\end{pmatrix}
\qquad \qquad
b =
\begin{pmatrix}
	-1 \\
	0
\end{pmatrix}
$$
Putting $x = (0)$ satisfies $A \* x \le b$.
Putting $y = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$ satisfies $(-A^T) \* y \le 0$ and $b \* y < 0$.
$$
A =
\begin{pmatrix}
	\bot & \top \\
	0 & -1
\end{pmatrix}
\qquad \qquad
b =
\begin{pmatrix}
	0 \\
	-1
\end{pmatrix}
$$
Putting $x = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$ satisfies $A \* x \le b$.
Putting $y = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$ satisfies $(-A^T) \* y \le 0$ and $b \* y < 0$.
$$
A =
\begin{pmatrix}
	\bot
\end{pmatrix}
\qquad \qquad
b =
\begin{pmatrix}
	\bot
\end{pmatrix}
$$
Putting $x = (1)$ satisfies $A \* x \le b$.
Putting $y = (0)$ satisfies $(-A^T) \* y \le 0$ and $b \* y < 0$.
$$
A =
\begin{pmatrix}
	\top \\
	-1
\end{pmatrix}
\qquad \qquad
b =
\begin{pmatrix}
	\top \\
	-1
\end{pmatrix}
$$
Putting $x = (1)$ satisfies $A \* x \le b$.
Putting $y = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$ satisfies $(-A^T) \* y \le 0$ and $b \* y < 0$.

\section {Proving the (extended) strong LP duality}

TODO.


\section {Related work}

TODO.


\section {Conclusion}

TODO.


\end{document}
